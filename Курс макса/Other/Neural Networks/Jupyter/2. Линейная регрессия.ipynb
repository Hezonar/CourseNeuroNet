{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия\n",
    "\n",
    "## Модель\n",
    "Рассмотрим первую простую модель линейной регрессии, которая выглядит так:\n",
    "$$\n",
    "y = \\sum_{i=1}^{n} w_ix_i + b,\n",
    "$$\n",
    "где $x_i$ - входы модели, $y$ - выход, $w_i$ и $b$ - параметры модели, которые мы будем обучать.\n",
    "\n",
    "Заметим, что сумма $\\sum_{i=1}^{n} w_ix_i$ является скалярным произведением векторов $\\vec{w} = (w_1, w_2, ..., w_n)$ и $\\vec{x} = (x_1, x_2, ..., x_n)$, поэтому нашу модель можно записать проще:\n",
    "\\begin{equation*}\n",
    "y = \\vec{w}^T \\vec{x} + b\n",
    "\\end{equation*}\n",
    "\n",
    "Проделаем, небольшой трюк, который ещё больше облегчит запись. Для этого перенесём смещение $b$ в конец вектора $\\vec{w}$, а в $\\vec{x}$ допишем фиктивную единицу, получим $\\vec{w} = (w_1, w_2, ..., w_n, b)$, $\\vec{x} = (x_1, x_2, ..., x_n, 1)$. Таким образом, придём к записи:\n",
    "\\begin{equation*}\n",
    "y = \\vec{w}^T \\vec{x}\n",
    "\\tag{1}\n",
    "\\end{equation*}\n",
    "В дальнейшем в качестве нашей модели будем использовать именно последнее вырадение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функционал потерь\n",
    "\n",
    "Пусть имеется выборка $X^{N} = (x_i)^{N}_{i=1}, Y^N = (y_i)^N_{i=1}$. Воспользуемся описанной ранее функцией **среднеквадратической ошибки (MSE - Mean Squarred Error)**:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^N (g(x_i|\\theta)-y_i)^2\n",
    "\\tag{2}\n",
    "\\end{equation*}$$,\n",
    "где $x_i$ - входной вектор размера n. $y_i$ - ответ в виде числа, $g(x_i|\\theta)$ - модель, $\\theta$ - параметры модели. Подставив линейную регрессию (1) вместо модели $g$ в наш функционал потерь получим:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^N (\\vec{w}^T \\vec{x_i}-y_i)^2\n",
    "\\tag{3}\n",
    "\\end{equation*}$$\n",
    "\n",
    "Большинство алгоритмов машинного обучения в том или ином виде включает оптимизацию, т.е. нахождение минимума или максимума функции f(x) при изменении x. Обычно задачу оптимизации формулируют в терминах нахождения минимума. Для нахождения максимума достаточно применить алгоритм минимизации к функции –f(x). Функция, для которой мы ищем минимум или максимум, называется *целевой функцией*. Если речь идет о минимизации, то употребляют также \n",
    "термины *функция стоимости*, *функция потерь* или *функция ошибок*. В этом курсе все эти термины будут использоваться как синонимы.\n",
    "\n",
    "Мы дожны спроектировать алгоритм машинного обучения, который, наблюдая обучающую выборку $X^N, Y^N$, улучшает веса модели $\\vec{w}$ таким образом, чтобы функционал качетва MSE был наименьшим. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Небольшое отступление\n",
    "\n",
    "Предполагается, что слушатели курса знакомы с основами математического анализа и линейной алгебры, но всё же сделаем краткий обзор некоторых необходимых понятий. \n",
    "\n",
    "Пусть дана дифференцируемая функция $f(x)$. Точки, в которых $f′(x) = 0$, называются *критическими*, или *стационарными*. \n",
    "\n",
    "*Локальным минимумом* называется точка, в которой $f(x)$ меньше, чем во всех точках \n",
    "малой окрестности, поэтому уменьшить $f(x)$ путем изменения аргумента $x$ на небольшую величину невозможно. *Локальным максимумом* называется точка, в которой $f(x)$ больше, чем во всех точках малой окрестности, поэтому невозможно увеличить $f(x)$ путем изменения аргумента $x$ на небольшую величину. Некоторые критические \n",
    "точки не являются ни минимумами, ни максимумами. Они называются *седловыми\n",
    "точками*. Далее на рис. показаны примеры всех критических точек.\n",
    "\n",
    "Для функций нескольких переменных следует ввести понятие частной производной. Пусть $f(x_1, x_2, ..., x_n)$ - функция от нескольких переменных. Для удобства введём вектор $x = (x_1, x_2, ..., x_n)$. \n",
    "\n",
    "*Частная производная* $\\delta/\\delta x_i f(x)$ показывает, как изменяется $f$ при изменении \n",
    "аргумента $x$ только в одном направлении $x_i$. *Градиент* обобщает понятие производной \n",
    "на вектор: *градиентом* функции $f$ называется вектор всех ее частных производных, \n",
    "он обозначается $\\nabla_x f(x)$. i-м элементом градиента является частная производная $f$ по \n",
    "$x_i$: \n",
    "\n",
    "$$\\nabla_x f(x) = (\\frac{\\delta f}{\\delta x_1}, \\frac{\\delta f}{\\delta x_2}, ..., \\frac{\\delta f}{\\delta x_n}).$$ \n",
    "\n",
    "В многомерном случае критическими называются точки, в которых все элементы \n",
    "градиента равны 0.\n",
    "\n",
    "И, наконец, запишем несколько полезных свойств градиента:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_w \\vec w^T \\vec a =  \\vec a\n",
    "\\tag{4}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_w \\vec w^T \\vec w = 2 \\vec w\n",
    "\\tag{5}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_{\\vec w} \\vec w^TA \\vec w = (A+A^T) \\vec w\n",
    "\\tag{6}\n",
    "\\end{equation*}\n",
    "\n",
    "Где $a, w$ - векторы длины $n$, $A$ - матрица $n \\times n$. Предлагается доказать данные свойства самостоятельно в качестве упражнения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Точная формула\n",
    "\n",
    "Для минимизации MSE мы можем просто приравнять градиент по w к 0 и решить получившееся уравнение:\n",
    "$$\\nabla_{\\vec{w}} MSE = 0$$\n",
    "$$\\nabla_{\\vec{w}} \\frac{1}{N} \\sum_{i=1}^N (\\vec{w}^T \\vec{x_i}-y_i)^2  = 0\n",
    "$$\n",
    "\n",
    "Перепишем уравнение в векторную форму, воспользовавшись умножением матриц $X=X^N, Y=Y^N$.\n",
    "$$\\nabla_{\\vec{w}}(X\\vec{w} - Y)^T(X\\vec{w}-Y)=0$$\n",
    "\n",
    "Раскрывая скобки, получаем:\n",
    "$$\\nabla_{\\vec{w}}(\\vec{w}^T X^T X \\vec w - \\vec w^TX^TY - Y^T \\vec w X + Y^T Y) = 0$$\n",
    "\n",
    "Возьмём градиент по $\\vec w$, пользуясь правилами (4)-(6).\n",
    "$$2X^T X \\vec w - 2X^T Y = 0$$\n",
    "$$\\vec w = (X^T X)^{-1} X^T Y$$\n",
    "\n",
    "Заметим, что обратная матрица $(X^T X)^{-1}$ может существовать не всегда. Для решения этой проблемы воспользуемся псевдоинверсией, обозначив её как $+$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\vec w = (X^T X)^+ X^T Y\n",
    "\\tag{7}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images\\gradient_descent_surf_example.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
