{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder\n",
    "\n",
    "Обучим модель переводу с американского анлийского на русский.\n",
    "\n",
    "[Воспользуемся датасетом Anki](http://www.manythings.org/anki/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = \"rus.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 73\n",
      "Number of unique output tokens: 85\n",
      "Max sequence length for inputs: 14\n",
      "Max sequence length for outputs: 60\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 10s 15ms/step - loss: 1.6629 - accuracy: 0.7461 - val_loss: 0.8896 - val_accuracy: 0.7637\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.7714 - accuracy: 0.7924 - val_loss: 0.7492 - val_accuracy: 0.8016\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.6553 - accuracy: 0.8277 - val_loss: 0.6527 - val_accuracy: 0.8219\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.5705 - accuracy: 0.8436 - val_loss: 0.5960 - val_accuracy: 0.8315\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.7568 - accuracy: 0.8243 - val_loss: 0.6657 - val_accuracy: 0.8192\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.5926 - accuracy: 0.8363 - val_loss: 0.6084 - val_accuracy: 0.8296\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.5471 - accuracy: 0.8457 - val_loss: 0.5861 - val_accuracy: 0.8315\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.5267 - accuracy: 0.8505 - val_loss: 0.5698 - val_accuracy: 0.8373\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.5096 - accuracy: 0.8544 - val_loss: 0.5560 - val_accuracy: 0.8418\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.4963 - accuracy: 0.8576 - val_loss: 0.5439 - val_accuracy: 0.8442\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.4826 - accuracy: 0.8603 - val_loss: 0.5318 - val_accuracy: 0.8461\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.4754 - accuracy: 0.8618 - val_loss: 0.5253 - val_accuracy: 0.8466\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.4645 - accuracy: 0.8644 - val_loss: 0.5148 - val_accuracy: 0.8510\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.4560 - accuracy: 0.8663 - val_loss: 0.5066 - val_accuracy: 0.8531\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.4472 - accuracy: 0.8691 - val_loss: 0.4978 - val_accuracy: 0.8547\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.4460 - accuracy: 0.8692 - val_loss: 0.4925 - val_accuracy: 0.8556\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.4346 - accuracy: 0.8728 - val_loss: 0.4879 - val_accuracy: 0.8572\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.4276 - accuracy: 0.8744 - val_loss: 0.4812 - val_accuracy: 0.8592\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.4208 - accuracy: 0.8765 - val_loss: 0.4768 - val_accuracy: 0.8602\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.4155 - accuracy: 0.8779 - val_loss: 0.4724 - val_accuracy: 0.8627\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.4093 - accuracy: 0.8807 - val_loss: 0.4649 - val_accuracy: 0.8642\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.3991 - accuracy: 0.8830 - val_loss: 0.4597 - val_accuracy: 0.8660\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.3916 - accuracy: 0.8857 - val_loss: 0.4651 - val_accuracy: 0.8644\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.3952 - accuracy: 0.8840 - val_loss: 0.4561 - val_accuracy: 0.8675\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.3818 - accuracy: 0.8884 - val_loss: 0.4487 - val_accuracy: 0.8698\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.3701 - accuracy: 0.8918 - val_loss: 0.4477 - val_accuracy: 0.8710\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.3719 - accuracy: 0.8913 - val_loss: 0.4421 - val_accuracy: 0.8719\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3600 - accuracy: 0.8947 - val_loss: 0.4443 - val_accuracy: 0.8727\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.3575 - accuracy: 0.8959 - val_loss: 0.4364 - val_accuracy: 0.8758\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3491 - accuracy: 0.8981 - val_loss: 0.4337 - val_accuracy: 0.8757\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3444 - accuracy: 0.8993 - val_loss: 0.4352 - val_accuracy: 0.8758\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.3387 - accuracy: 0.9014 - val_loss: 0.4292 - val_accuracy: 0.8778\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.3321 - accuracy: 0.9027 - val_loss: 0.4233 - val_accuracy: 0.8791\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.3303 - accuracy: 0.9036 - val_loss: 0.4209 - val_accuracy: 0.8799\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.3198 - accuracy: 0.9067 - val_loss: 0.4195 - val_accuracy: 0.8810\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.3149 - accuracy: 0.9079 - val_loss: 0.4208 - val_accuracy: 0.8795\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.3096 - accuracy: 0.9096 - val_loss: 0.4152 - val_accuracy: 0.8816\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.3050 - accuracy: 0.9104 - val_loss: 0.4155 - val_accuracy: 0.8821\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.2981 - accuracy: 0.9126 - val_loss: 0.4150 - val_accuracy: 0.8821\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.2947 - accuracy: 0.9133 - val_loss: 0.4125 - val_accuracy: 0.8831\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 3s 21ms/step - loss: 0.2854 - accuracy: 0.9162 - val_loss: 0.4087 - val_accuracy: 0.8847\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.2831 - accuracy: 0.9167 - val_loss: 0.4097 - val_accuracy: 0.8845\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.2778 - accuracy: 0.9186 - val_loss: 0.4105 - val_accuracy: 0.8848\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 3s 20ms/step - loss: 0.2711 - accuracy: 0.9204 - val_loss: 0.4066 - val_accuracy: 0.8848\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.2677 - accuracy: 0.9214 - val_loss: 0.4031 - val_accuracy: 0.8857\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.2614 - accuracy: 0.9227 - val_loss: 0.4058 - val_accuracy: 0.8859\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.2542 - accuracy: 0.9247 - val_loss: 0.4054 - val_accuracy: 0.8867\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.2504 - accuracy: 0.9260 - val_loss: 0.4067 - val_accuracy: 0.8871\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.2432 - accuracy: 0.9281 - val_loss: 0.4039 - val_accuracy: 0.8874\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.2384 - accuracy: 0.9297 - val_loss: 0.4030 - val_accuracy: 0.8882\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.2367 - accuracy: 0.9303 - val_loss: 0.4010 - val_accuracy: 0.8893\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.2316 - accuracy: 0.9318 - val_loss: 0.4039 - val_accuracy: 0.8885\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.2269 - accuracy: 0.9332 - val_loss: 0.4043 - val_accuracy: 0.8888\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.2200 - accuracy: 0.9348 - val_loss: 0.4045 - val_accuracy: 0.8890\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.2169 - accuracy: 0.9355 - val_loss: 0.4037 - val_accuracy: 0.8901\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.2115 - accuracy: 0.9372 - val_loss: 0.4063 - val_accuracy: 0.8899\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.2053 - accuracy: 0.9391 - val_loss: 0.4049 - val_accuracy: 0.8906\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.2009 - accuracy: 0.9400 - val_loss: 0.4071 - val_accuracy: 0.8904\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1969 - accuracy: 0.9416 - val_loss: 0.4048 - val_accuracy: 0.8915\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1946 - accuracy: 0.9421 - val_loss: 0.4116 - val_accuracy: 0.8906\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1885 - accuracy: 0.9439 - val_loss: 0.4120 - val_accuracy: 0.8910\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.1853 - accuracy: 0.9449 - val_loss: 0.4161 - val_accuracy: 0.8907\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1800 - accuracy: 0.9467 - val_loss: 0.4148 - val_accuracy: 0.8910\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1770 - accuracy: 0.9477 - val_loss: 0.4219 - val_accuracy: 0.8894\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1739 - accuracy: 0.9484 - val_loss: 0.4232 - val_accuracy: 0.8901\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1702 - accuracy: 0.9493 - val_loss: 0.4231 - val_accuracy: 0.8911\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.1644 - accuracy: 0.9507 - val_loss: 0.4241 - val_accuracy: 0.8910\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1613 - accuracy: 0.9517 - val_loss: 0.4258 - val_accuracy: 0.8917\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1580 - accuracy: 0.9530 - val_loss: 0.4297 - val_accuracy: 0.8909\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.1529 - accuracy: 0.9543 - val_loss: 0.4266 - val_accuracy: 0.8920\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.1484 - accuracy: 0.9553 - val_loss: 0.4402 - val_accuracy: 0.8898\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1442 - accuracy: 0.9566 - val_loss: 0.4379 - val_accuracy: 0.8920\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1433 - accuracy: 0.9572 - val_loss: 0.4393 - val_accuracy: 0.8918\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1400 - accuracy: 0.9582 - val_loss: 0.4446 - val_accuracy: 0.8916\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1370 - accuracy: 0.9588 - val_loss: 0.4440 - val_accuracy: 0.8914\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.1314 - accuracy: 0.9605 - val_loss: 0.4523 - val_accuracy: 0.8913\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1304 - accuracy: 0.9609 - val_loss: 0.4530 - val_accuracy: 0.8919\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1277 - accuracy: 0.9617 - val_loss: 0.4561 - val_accuracy: 0.8912\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.1237 - accuracy: 0.9628 - val_loss: 0.4580 - val_accuracy: 0.8923\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1234 - accuracy: 0.9632 - val_loss: 0.4604 - val_accuracy: 0.8916\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1176 - accuracy: 0.9647 - val_loss: 0.4633 - val_accuracy: 0.8914\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1169 - accuracy: 0.9645 - val_loss: 0.4707 - val_accuracy: 0.8906\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1126 - accuracy: 0.9658 - val_loss: 0.4766 - val_accuracy: 0.8910\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1081 - accuracy: 0.9675 - val_loss: 0.4791 - val_accuracy: 0.8909\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1081 - accuracy: 0.9674 - val_loss: 0.4803 - val_accuracy: 0.8909\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1062 - accuracy: 0.9681 - val_loss: 0.4921 - val_accuracy: 0.8908\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.1021 - accuracy: 0.9689 - val_loss: 0.4867 - val_accuracy: 0.8913\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.1006 - accuracy: 0.9693 - val_loss: 0.4950 - val_accuracy: 0.8906\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0960 - accuracy: 0.9709 - val_loss: 0.4939 - val_accuracy: 0.8901\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0959 - accuracy: 0.9703 - val_loss: 0.5025 - val_accuracy: 0.8905\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.0930 - accuracy: 0.9717 - val_loss: 0.5019 - val_accuracy: 0.8907\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0910 - accuracy: 0.9722 - val_loss: 0.5147 - val_accuracy: 0.8901\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0896 - accuracy: 0.9724 - val_loss: 0.5117 - val_accuracy: 0.8903\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0873 - accuracy: 0.9734 - val_loss: 0.5165 - val_accuracy: 0.8908\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0832 - accuracy: 0.9746 - val_loss: 0.5166 - val_accuracy: 0.8908\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0823 - accuracy: 0.9746 - val_loss: 0.5229 - val_accuracy: 0.8903\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.0794 - accuracy: 0.9755 - val_loss: 0.5263 - val_accuracy: 0.8892\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0801 - accuracy: 0.9747 - val_loss: 0.5303 - val_accuracy: 0.8903\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0761 - accuracy: 0.9764 - val_loss: 0.5313 - val_accuracy: 0.8907\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.0746 - accuracy: 0.9768 - val_loss: 0.5374 - val_accuracy: 0.8895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = keras.models.load_model(\"s2s\")\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Иди.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здоро́во!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Беги!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Круто!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Огонь!\n",
      "\n",
      "-\n",
      "Input sentence: We loved you.\n",
      "Decoded sentence: Мы вас любим.\n",
      "\n",
      "-\n",
      "Input sentence: We might die.\n",
      "Decoded sentence: Мы солжны пойти.\n",
      "\n",
      "-\n",
      "Input sentence: We must talk.\n",
      "Decoded sentence: Мы должны бежать.\n",
      "\n",
      "-\n",
      "Input sentence: We need more.\n",
      "Decoded sentence: Ты нам нужен.\n",
      "\n",
      "-\n",
      "Input sentence: We need this.\n",
      "Decoded sentence: Она нам нужна.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(0, 25, 5):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)\n",
    "\n",
    "\n",
    "for seq_index in range(8600, 8625, 5):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "data_path = \"rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, \"r\", encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')[:2]]\n",
    "                for line in lines[:min(num_examples, len(lines)-1)]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 100000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(data_path,\n",
    "                                                                num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> беги ! <end>']\n",
      "['<start> run . <end>']\n"
     ]
    }
   ],
   "source": [
    "print(inp_lang.sequences_to_texts(input_tensor[10:11]))\n",
    "print(targ_lang.sequences_to_texts(target_tensor[10:11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 15]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 15, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
    "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 7274)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.0507\n",
      "Time taken for 1 epoch 88.88 sec\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e4b6d17fe181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maxifer/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maxifer/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maxifer/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maxifer/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maxifer/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/maxifer/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "    \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 4 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование и визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff3b7672a50>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = ''\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore('./training_checkpoints/ckpt-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_inp = input_tensor.shape[1]\n",
    "max_length_targ = target_tensor.shape[1]\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input:', sentence)\n",
    "    print('Predicted translation:', result)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')),\n",
    "                                  :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ты всё ещё дома ? <end>\n",
      "Predicted translation: are you still at home ? <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'ты всё ещё дома?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> здесь довольно холодно <end>\n",
      "Predicted translation: it's pretty cold here ? <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'здесь довольно холодно')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> холодно здесь <end>\n",
      "Predicted translation: it's cold in here ? <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'холодно здесь')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ссылки\n",
    "- https://keras.io/examples/nlp/lstm_seq2seq/\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
